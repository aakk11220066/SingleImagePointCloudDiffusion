{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e36aa9-bbf1-4faf-a85c-64203a1db7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import types\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ModelDiffusion import get_default_diffusion_model\n",
    "from ModelDiffusion.diffusion.utils.dataset import ShapeNetCore\n",
    "from DiffRender import get_default_rasterizer\n",
    "# from Wasserstein.SlicedWassersteinDistance.swd import swd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a9934-7d96-4105-92d1-3925b5759273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_interactive():\n",
    "    return True\n",
    "    return not hasattr(__main__, \"__file__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f80e2-156a-4cfe-b7b8-d8c22b85a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    diffusion_context_input_dim=256,\n",
    "    diffusion_context_hidden_dim=256,\n",
    "    dataset_path='./ModelDiffusion/diffusion/data/shapenet.hdf5',\n",
    "    categories=['airplane'],\n",
    "    scale_mode='shape_unit',\n",
    "    train_batch_size=32,\n",
    "    val_batch_size=8,\n",
    "    sample_num_points=2048,\n",
    "    max_test_comparisons=10,\n",
    "    normalize=None,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3383ac-967d-4a65-9dfd-de4cb21812d2",
   "metadata": {},
   "source": [
    "## Image encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486c599-c8e3-48a3-aacb-9192b758d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_encoder = torchvision.models.densenet121(weights=torchvision.models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "densenet_encoder.classifier = nn.Sequential(\n",
    "    nn.Linear(\n",
    "        in_features=densenet_encoder.classifier.in_features, \n",
    "        out_features=args.diffusion_context_hidden_dim\n",
    "    ),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(\n",
    "        in_features=args.diffusion_context_hidden_dim, \n",
    "        out_features=args.diffusion_context_input_dim\n",
    "    )\n",
    ")\n",
    "densenet_encoder = densenet_encoder.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9afe2ff-59f4-40b7-a444-db445ceed70d",
   "metadata": {},
   "source": [
    "## Image reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784946ba-bfc0-4534-944c-9434d72d4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "class ImageReconstructionModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_encoder=densenet_encoder,\n",
    "        render_depthmap=get_default_rasterizer(img_height=256, img_width=256), \n",
    "        diffusion_model=get_default_diffusion_model(sample_num_points=args.sample_num_points, device=args.device, category=args.categories[0]),\n",
    "        loss_fn=lambda reconstruction, ground_truth: swd(reconstruction, ground_truth, device=args.device)\n",
    "    ):\n",
    "        # Input:\n",
    "        # :img_encoder: function that receives batch of images (shape: BATCH_SIZE x IMG_HEIGHT x IMG_WIDTH) \n",
    "        #     and returns batch of encodings (shape: BATCH_SIZE x ENCODING_SIZE)\n",
    "        # :render_depthmap: function that receives batch of pointclouds \n",
    "        #     (shape: BATCH_SIZE x NUM_POINTCLOUD_POINTS x POINTCLOUD_DIM) and returns batch of photographs of pointclouds\n",
    "        #     (shape: BATCH_SIZE x NUM_POINTCLOUD_POINTS x POINTCLOUD_DIM)\n",
    "        # :diffusion_model: function that receives context (shape: BATCH_SIZE x ENCODING_SIZE) and returns batch of pointclouds\n",
    "        #     (shape: BATCH_SIZE x NUM_POINTCLOUD_POINTS x POINTCLOUD_DIM)\n",
    "        # :loss_fn: function that receives batch of image reconstructions (shape: BATCH_SIZE x IMG_HEIGHT x IMG_WIDTH) and \n",
    "        #     batch of image ground truths (shape: BATCH_SIZE x IMG_HEIGHT x IMG_WIDTH) and returns batch of scalar losses\n",
    "        \n",
    "        super().__init__()\n",
    "        self.loss_fn = loss_fn.to(self.device)\n",
    "        self.render_depthmap = render_depthmap.to(self.device)\n",
    "        self.img_encoder = img_encoder.to(self.device)\n",
    "        self.diffusion_model = diffusion_model.to(self.device)\n",
    "        \n",
    "    def pointcloud_denoising_comparison(self, pointcloud_batch, denoising_step_number):\n",
    "        # Fetch noising parameters\n",
    "        alpha_bar = self.diffusion_model.var_sched.alpha_bars[denoising_step_number]\n",
    "        c0 = torch.sqrt(alpha_bar).view(-1, 1, 1)       # (B, 1, 1)\n",
    "        c1 = torch.sqrt(1 - alpha_bar).view(-1, 1, 1)   # (B, 1, 1)\n",
    "        \n",
    "        alpha_bar_prev = self.diffusion_model.var_sched.alpha_bars[denoising_step_number-1]\n",
    "        c0_prev = torch.sqrt(alpha_bar).view(-1, 1, 1)       # (B, 1, 1)\n",
    "        c1_prev = torch.sqrt(1 - alpha_bar).view(-1, 1, 1)   # (B, 1, 1)\n",
    "        c0_prev[torch.where(denoising_step_number == 0)] = 1\n",
    "        c1_prev[torch.where(denoising_step_number == 0)] = 0\n",
    "\n",
    "        # Noise pointcloud\n",
    "        e_rand = torch.randn_like(pointcloud_batch)  # shape: BATCH_SIZE x NUM_POINTCLOUD_POINTS x POINTCLOUD_DIM\n",
    "        groundtruth_pointcloud = c0_prev * pointcloud_batch + c1_prev * e_rand\n",
    "        noised_pointcloud = c0 * pointcloud_batch + c1 * e_rand\n",
    "        \n",
    "        return groundtruth_pointcloud, noised_pointcloud\n",
    "        \n",
    "    def training_step(self, pointcloud_batch, batch_idx, denoising_step_number=None):\n",
    "        # Input: pointcloud_batch with shape BATCH_SIZE x NUM_POINTCLOUD_POINTS x POINTCLOUD_DIM\n",
    "        # Performs diffusion-step loss, by comparing image of noised pointcloud against image of slightly denoised \n",
    "        #     image of pointcloud, with that slight denoising being performed by the context-informed diffusion network.\n",
    "        #     Parallel to self.diffusion_model.get_loss\n",
    "        pointcloud_batch = pointcloud_batch.to(args.device)\n",
    "        batch_size, _, point_dim = pointcloud_batch.size()\n",
    "        if denoising_step_number == None:\n",
    "            denoising_step_number = self.diffusion_model.var_sched.uniform_sample_t(batch_size)\n",
    "        \n",
    "        groundtruth_pointcloud, noised_pointcloud = self.pointcloud_denoising_comparison(\n",
    "            pointcloud_batch, \n",
    "            torch.tensor(denoising_step_number)\n",
    "        )\n",
    "        \n",
    "        # Estimate partially denoised pointcloud\n",
    "        origin_img_renders = self.render_depthmap(pointcloud_batch)  # shape: BATCH_SIZE x IMG_HEIGHT x IMG_WIDTH\n",
    "        \n",
    "        encoded_imgs = self.img_encoder(origin_img_renders.unsqueeze(1).repeat(1,3,1,1))  # shape: BATCH_SIZE x ENCODING_SIZE\n",
    "        model_pointcloud_reconstructions = self.diffusion_model(\n",
    "            noised_pointcloud=noised_pointcloud,\n",
    "            context=encoded_imgs, \n",
    "            denoising_step_number=denoising_step_number\n",
    "        )  # shape: BATCH_SIZE x NUM_POINTCLOUD_POINTS x POINTCLOUD_DIM\n",
    "        reconstruction_img_renders = self.render_depthmap(model_pointcloud_reconstructions)  # shape: BATCH_SIZE x IMG_HEIGHT x IMG_WIDTH\n",
    "\n",
    "        # Measure success\n",
    "        groundtruth_img_renders = self.render_depthmap(groundtruth_pointcloud)\n",
    "        loss = self.loss_fn(\n",
    "            # reconstruction_img_renders.unsqueeze(1).repeat(1,3,1,1),  for Wasserstein loss\n",
    "            # groundtruth_img_renders.unsqueeze(1).repeat(1,3,1,1)    for Wasserstein loss\n",
    "            reconstruction_img_renders,\n",
    "            groundtruth_img_renders\n",
    "        )\n",
    "        self.log('train_loss', loss.mean())\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, pointcloud_batch, batch_idx, visualize_validation=False):\n",
    "        pointcloud_batch = pointcloud_batch.to(args.device)\n",
    "        \n",
    "        groundtruth_img_renders = self.render_depthmap(pointcloud_batch)  # shape: BATCH_SIZE x IMG_HEIGHT x IMG_WIDTH\n",
    "        model_pointcloud_reconstructions = self.forward(img=groundtruth_img_renders) # shape: BATCH_SIZE x NUM_POINTCLOUD_POINTS x POINTCLOUD_DIM\n",
    "        reconstruction_img_renders = self.render_depthmap(model_pointcloud_reconstructions)  # shape: BATCH_SIZE x IMG_HEIGHT x IMG_WIDTH\n",
    "\n",
    "        loss = self.loss_fn(\n",
    "            # reconstruction_img_renders.unsqueeze(1).repeat(1,3,1,1),  for Wasserstein loss\n",
    "            # groundtruth_img_renders.unsqueeze(1).repeat(1,3,1,1)    for Wasserstein loss\n",
    "            reconstruction_img_renders,\n",
    "            groundtruth_img_renders\n",
    "        )\n",
    "        self.log(\"validation_loss\", loss.mean())\n",
    "\n",
    "        # Visualize model\n",
    "        if visualize_validation and batch_idx == 0:\n",
    "            global i\n",
    "            fig, axes = plt.subplots(ncols=2, figsize=(15,15))\n",
    "            fig.suptitle(f\"Sample #{batch_idx}\")\n",
    "            axes[0].imshow(groundtruth_img_renders[0].detach().cpu().numpy())\n",
    "            axes[0].set_title(\"Ground truth\")\n",
    "            axes[1].imshow(reconstruction_img_renders[0].detach().cpu().numpy())\n",
    "            axes[1].set_title(\"Reconstruction\")\n",
    "            fig.savefig(f\"Comparisons/Img{i}.png\")\n",
    "            i += 1\n",
    "        \n",
    "    def test_step(self, pointcloud_batch, batch_idx):\n",
    "        pointcloud_batch = pointcloud_batch.to(args.device)\n",
    "        \n",
    "        groundtruth_img_renders = self.render_depthmap(pointcloud_batch)  # shape: BATCH_SIZE x IMG_HEIGHT x IMG_WIDTH\n",
    "        model_pointcloud_reconstructions = self.forward(img=groundtruth_img_renders) # shape: BATCH_SIZE x NUM_POINTCLOUD_POINTS x POINTCLOUD_DIM\n",
    "        reconstruction_img_renders = self.render_depthmap(model_pointcloud_reconstructions)  # shape: BATCH_SIZE x IMG_HEIGHT x IMG_WIDTH\n",
    "\n",
    "        return groundtruth_img_renders, reconstruction_img_renders\n",
    "        \n",
    "    def forward(self, img):\n",
    "        with torch.no_grad():\n",
    "            encoded_img = self.img_encoder(img.unsqueeze(1).repeat(1,3,1,1))  # shape: BATCH_SIZE x ENCODING_SIZE\n",
    "            generated_pointcloud = self.diffusion_model.sample(context=encoded_img)  # shape: BATCH_SIZE x NUM_POINTCLOUD_POINTS x POINTCLOUD_DIM\n",
    "        return generated_pointcloud\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.NAdam(self.img_encoder.parameters())\n",
    "    \n",
    "    def train_model(\n",
    "        self, \n",
    "        datamodule, \n",
    "        checkpoint_file_path=None, \n",
    "        gpus=None, \n",
    "        max_epochs=2000,\n",
    "        wandb_run_name=None,\n",
    "        wandb_group_name=None,\n",
    "        pretrained_model_path=None,\n",
    "        wandb_config={\"project\": None, \"entity\": None}\n",
    "    ):\n",
    "        ALL_GPUS = -1\n",
    "        gpus = ALL_GPUS if gpus is None else gpus\n",
    "        \n",
    "        model_checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=checkpoint_file_path, \n",
    "            save_top_k=1, \n",
    "            monitor='validation_loss', \n",
    "            mode='min', \n",
    "            save_on_train_epoch_end=True\n",
    "        )\n",
    "        lr_logger = pl.callbacks.LearningRateMonitor(log_momentum=True)\n",
    "        callbacks_list = [model_checkpoint, lr_logger]\n",
    "\n",
    "        wandb_logger = pl.loggers.WandbLogger(project=wandb_config[\"project\"], entity=wandb_config[\"entity\"], name=wandb_run_name, group=wandb_group_name)\n",
    "        self.log('batch_size', args.train_batch_size)\n",
    "        # wandb_logger.watch(self, log='all', log_freq=100)\n",
    "        if type(wandb_logger.experiment.config) != types.MethodType:  # if this is main process (as opposed to launched by ddp)\n",
    "            wandb_logger.experiment.config[\"model_name\"] = self.__class__.__name__\n",
    "            wandb_logger.experiment.config[\"img_encoder\"] = self.img_encoder.__class__.__name__\n",
    "            wandb_logger.experiment.config[\"model_architecture\"] = self.__str__()\n",
    "\n",
    "        print(f\"parallelization strategy = {'dp' if is_interactive() else 'ddp_find_unused_parameters_false'}\")\n",
    "        trainer = pl.Trainer(\n",
    "            devices=gpus, \n",
    "            accelerator=\"gpu\", \n",
    "            strategy=\"dp\" if is_interactive() else \"ddp_find_unused_parameters_false\", \n",
    "            precision=32, \n",
    "            max_epochs=max_epochs, \n",
    "            logger=wandb_logger, \n",
    "            callbacks=callbacks_list, \n",
    "            num_sanity_val_steps=1,\n",
    "            log_every_n_steps=5\n",
    "        )\n",
    "        trainer.fit(self, datamodule, ckpt_path=pretrained_model_path)\n",
    "        print(f\"Best model checkpoint saved at: {callbacks_list[0].best_model_path}\")\n",
    "\n",
    "        wandb_logger.experiment.finish()\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1e278-783e-4b2c-a6c3-65702bca0ca2",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d178e-29c5-4b2d-9133-1b03d21f5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapenetDatamodule(pl.LightningDataModule):\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=ShapeNetCore(\n",
    "                path=args.dataset_path,\n",
    "                cates=args.categories,\n",
    "                split='train',\n",
    "                scale_mode=args.scale_mode,\n",
    "            ),\n",
    "            shuffle=True,\n",
    "            batch_size=args.train_batch_size,\n",
    "            num_workers=80\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=ShapeNetCore(\n",
    "                path=args.dataset_path,\n",
    "                cates=args.categories,\n",
    "                split='val',\n",
    "                scale_mode=args.scale_mode,\n",
    "            ),\n",
    "            shuffle=False,\n",
    "            batch_size=args.val_batch_size,\n",
    "            num_workers=80\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=ShapeNetCore(\n",
    "                path=args.dataset_path,\n",
    "                cates=args.categories,\n",
    "                split='test',\n",
    "                scale_mode=args.scale_mode,\n",
    "            ),\n",
    "            shuffle=False,\n",
    "            batch_size=1,\n",
    "            num_workers=80\n",
    "        )\n",
    "    \n",
    "shapenet_datamodule = ShapenetDatamodule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4e90a-8b51-497d-83fc-520d6bc0c01a",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f1906-2fc9-4238-9970-dfb8b89429c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageReconstructionModel(loss_fn=nn.HuberLoss())  # Future direction for more compute power: Wasserstsein loss\n",
    "trainer = model.train_model(shapenet_datamodule, gpus=None, wandb_run_name=\"Run2\", pretrained_model_path=\"3dDiffusion_ImgMatching/v8xe37f8/checkpoints/epoch=29-step=3240.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a4449-a183-430c-b6e4-a886450ca19c",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ed734-3002-4ade-92fb-3d800dddfb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, pointcloud_batch in enumerate(shapenet_datamodule.test_dataloader()):\n",
    "    pointcloud_batch = pointcloud_batch.to(args.device)\n",
    "    \n",
    "    groundtruth_img, reconstructed_img = model.test_step(pointcloud_batch, batch_idx)\n",
    "    groundtruth_img, reconstructed_img = groundtruth_img.squeeze(), reconstructed_img.squeeze()\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(15,15))\n",
    "    fig.title(f\"Sample #{batch_idx}\")\n",
    "    axes[0].imshow(groundtruth_img)\n",
    "    axes[0].set_title(\"Ground truth\")\n",
    "    axes[1].imshow(reconstructed_img)\n",
    "    axes[1].set_title(\"Reconstruction\")\n",
    "    fig.show()\n",
    "    \n",
    "    if batch_idx >= args.max_test_comparisons:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (base_env)",
   "language": "python",
   "name": "base-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
